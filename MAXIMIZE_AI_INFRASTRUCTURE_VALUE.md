# üöÄ Maximizing Value from Existing AI Infrastructure

**Date**: 2025-11-04
**Purpose**: Leverage ALL existing AI/RLHF/training tables for continuous improvement
**Goal**: Build self-improving system that gets smarter with every user interaction

---

## üéØ Executive Summary

You have **13 powerful AI tables** already in Supabase that are **mostly unused**:

### ‚úÖ Currently Active (3 tables)
1. `ai_response_cache` - Cost optimization ‚úÖ **NOW USING**
2. `ai_knowledge_sources` - RAG knowledge base ‚úÖ **PARTIALLY USING**
3. `brand_assets` - Coach personas ‚úÖ **USING**

### üíé **Goldmine of Unused Value** (10 tables)
4. `ai_training_datasets` - RLHF data collection üí∞ **HIGH ROI**
5. `ai_training_examples` - Individual training samples üí∞ **HIGH ROI**
6. `ai_finetuning_jobs` - Track fine-tuning runs üí∞ **MEDIUM ROI**
7. `ai_model_versions` - A/B test model performance üí∞ **MEDIUM ROI**
8. `ai_context_cache` - User context optimization üí∞ **HIGH ROI**
9. `ai_episodic_memory` - Long-term user memory üí∞ **HIGH ROI**
10. `ai_predictions` - Track prediction accuracy üí∞ **MEDIUM ROI**
11. `ai_experiments` - A/B testing framework üí∞ **MEDIUM ROI**
12. `ai_errors` - Error tracking + debugging üí∞ **LOW ROI**
13. `food_photo_training_data` - Food recognition training üí∞ **MEDIUM ROI**

---

## üî• **PRIORITY 1: Reinforcement Learning from Human Feedback (RLHF)**

### What It Is
Collect user feedback on every AI response ‚Üí Use it to fine-tune your own model ‚Üí Get better over time

### Why Maximum ROI
‚úÖ **Free Training Data**: Users generate training data as they chat
‚úÖ **Continuous Improvement**: Model gets better daily
‚úÖ **Cost Reduction**: Fine-tuned model cheaper than GPT-4o
‚úÖ **Competitive Moat**: Your model learns YOUR users' preferences

### Tables Involved
- `ai_training_datasets` - Organize training data
- `ai_training_examples` - Store individual conversations
- `coach_response_feedback` (from Figma First) - User ratings

---

### Implementation Plan (3 Days)

#### Day 1: Capture Feedback ‚≠ê **START HERE**

##### 1. Add Feedback Table (Additive)

```sql
CREATE TABLE IF NOT EXISTS coach_response_feedback (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID NOT NULL REFERENCES auth.users(id),
  coach_id TEXT,

  -- The conversation
  user_message TEXT NOT NULL,
  ai_response TEXT NOT NULL,
  system_prompt TEXT,  -- What prompt generated this
  severity DECIMAL(2,1),
  mode TEXT,

  -- User feedback
  helpful BOOLEAN,  -- Thumbs up/down
  rating INT CHECK (rating BETWEEN 1 AND 5),  -- Optional 5-star rating
  feedback_text TEXT,  -- Optional written feedback

  -- Context
  conversation_id UUID,
  response_time_ms INT,
  tokens_used INT,

  created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_feedback_helpful ON coach_response_feedback(helpful) WHERE helpful IS NOT NULL;
CREATE INDEX idx_feedback_rating ON coach_response_feedback(rating) WHERE rating IS NOT NULL;
CREATE INDEX idx_feedback_coach ON coach_response_feedback(coach_id);
```

##### 2. Update Edge Function to Log Responses

```typescript
// After getting AI response, log it for potential feedback
const { data: logEntry } = await supabase
  .from('coach_response_feedback')
  .insert({
    user_id: userId,
    coach_id: coachId,
    user_message: userMessage,
    ai_response: aiResponse,
    system_prompt: systemPrompt,
    severity: severity,
    mode: mode,
    response_time_ms: responseTime,
    tokens_used: tokensUsed
  })
  .select('id')
  .single()

// Return response with feedback ID
return {
  message: aiResponse,
  feedbackId: logEntry.id  // Frontend uses this for thumbs up/down
}
```

##### 3. Add Thumbs Up/Down UI (Vibe AI)

```typescript
// After each AI message
<View className="flex-row gap-2 mt-2">
  <Pressable onPress={() => submitFeedback(feedbackId, true)}>
    <Ionicons name="thumbs-up" size={20} />
  </Pressable>
  <Pressable onPress={() => submitFeedback(feedbackId, false)}>
    <Ionicons name="thumbs-down" size={20} />
  </Pressable>
</View>

const submitFeedback = async (feedbackId: string, helpful: boolean) => {
  await supabase
    .from('coach_response_feedback')
    .update({ helpful })
    .eq('id', feedbackId)
}
```

**Result**: You now capture user feedback on every response! üéâ

---

#### Day 2: Auto-Create Training Dataset

##### 1. Add Function to Convert Feedback ‚Üí Training Examples

```sql
-- Automatically convert thumbs-up responses to training data
CREATE OR REPLACE FUNCTION generate_training_examples_from_feedback()
RETURNS VOID
LANGUAGE plpgsql
SECURITY DEFINER
AS $$
DECLARE
  v_dataset_id UUID;
  v_positive_count INT;
BEGIN
  -- Get or create "User Feedback - Positive" dataset
  SELECT id INTO v_dataset_id
  FROM ai_training_datasets
  WHERE dataset_name = 'User Feedback - Positive Responses';

  IF NOT FOUND THEN
    INSERT INTO ai_training_datasets (
      dataset_name,
      description,
      model_target,
      dataset_type,
      status
    ) VALUES (
      'User Feedback - Positive Responses',
      'AI responses that users marked as helpful (thumbs up)',
      'gpt-4o-mini',  -- Fine-tune cheaper model
      'conversation',
      'collecting'
    )
    RETURNING id INTO v_dataset_id;
  END IF;

  -- Insert positive feedback as training examples
  INSERT INTO ai_training_examples (
    dataset_id,
    messages,
    quality_score,
    user_satisfaction,
    is_validated,
    source_type,
    source_id,
    split
  )
  SELECT
    v_dataset_id,
    jsonb_build_array(
      jsonb_build_object('role', 'system', 'content', crf.system_prompt),
      jsonb_build_object('role', 'user', 'content', crf.user_message),
      jsonb_build_object('role', 'assistant', 'content', crf.ai_response)
    ),
    CASE
      WHEN crf.rating IS NOT NULL THEN crf.rating * 2  -- Scale 1-5 to 2-10
      ELSE 7.0  -- Default quality for thumbs up
    END,
    1.0,  -- User satisfied (gave thumbs up)
    true,  -- Validated by real user
    'user_conversation',
    crf.id,
    CASE
      WHEN RANDOM() < 0.8 THEN 'train'
      WHEN RANDOM() < 0.9 THEN 'validation'
      ELSE 'test'
    END
  FROM coach_response_feedback crf
  WHERE crf.helpful = TRUE
    AND crf.rating IS NULL OR crf.rating >= 4  -- 4-5 star ratings
    AND NOT EXISTS (
      -- Don't duplicate
      SELECT 1 FROM ai_training_examples
      WHERE source_id = crf.id
    );

  -- Update dataset stats
  SELECT COUNT(*) INTO v_positive_count
  FROM ai_training_examples
  WHERE dataset_id = v_dataset_id;

  UPDATE ai_training_datasets
  SET
    total_examples = v_positive_count,
    train_examples = (SELECT COUNT(*) FROM ai_training_examples WHERE dataset_id = v_dataset_id AND split = 'train'),
    validation_examples = (SELECT COUNT(*) FROM ai_training_examples WHERE dataset_id = v_dataset_id AND split = 'validation'),
    test_examples = (SELECT COUNT(*) FROM ai_training_examples WHERE dataset_id = v_dataset_id AND split = 'test'),
    contains_user_feedback = TRUE,
    average_quality_score = (SELECT AVG(quality_score) FROM ai_training_examples WHERE dataset_id = v_dataset_id)
  WHERE id = v_dataset_id;

  RAISE NOTICE 'Generated % training examples from positive feedback', v_positive_count;
END;
$$;

-- Run this daily via cron
SELECT cron.schedule(
  'generate-training-data',
  '0 2 * * *',  -- 2 AM daily
  $$SELECT generate_training_examples_from_feedback()$$
);
```

**Result**: Every thumbs-up becomes training data automatically! üîÑ

---

#### Day 3: Export & Fine-Tune

##### 1. Export Function (OpenAI Format)

```sql
CREATE OR REPLACE FUNCTION export_training_dataset_openai_format(
  p_dataset_id UUID
)
RETURNS TABLE (
  training_data JSONB
)
LANGUAGE plpgsql
AS $$
BEGIN
  RETURN QUERY
  SELECT jsonb_agg(
    jsonb_build_object(
      'messages', ate.messages
    )
  )
  FROM ai_training_examples ate
  WHERE ate.dataset_id = p_dataset_id
    AND ate.split = 'train'
    AND ate.is_validated = TRUE;
END;
$$;
```

##### 2. Export Script (Run Monthly)

```typescript
// scripts/export-training-data.ts
const { data } = await supabase.rpc('export_training_dataset_openai_format', {
  p_dataset_id: 'your-dataset-id'
})

const trainingFile = data[0].training_data

// Save to file
await Deno.writeTextFile(
  './training_data.jsonl',
  trainingFile.map(item => JSON.stringify(item)).join('\n')
)

console.log('Training data exported! Ready for OpenAI fine-tuning.')
```

##### 3. Fine-Tune via OpenAI (Monthly)

```bash
# Upload training file
openai files create -f training_data.jsonl -p fine-tune

# Create fine-tuning job
openai fine_tuning.jobs.create \
  -t file-abc123 \
  -m gpt-4o-mini-2024-07-18 \
  --suffix "mindfork-roast-coach-v1"

# Wait for completion (takes 1-24 hours)
# Result: ft:gpt-4o-mini:mindfork::abc123
```

##### 4. Track Fine-Tuning Job

```sql
-- Log fine-tuning job
INSERT INTO ai_finetuning_jobs (
  dataset_id,
  job_name,
  base_model,
  external_job_id,
  status,
  estimated_cost
) VALUES (
  'dataset-uuid',
  'MindFork Roast Coach v1',
  'gpt-4o-mini',
  'ftjob-abc123',
  'running',
  25.00
);

-- When complete, create model version
INSERT INTO ai_model_versions (
  finetuning_job_id,
  model_name,
  version,
  model_id,
  is_deployed,
  deployment_environment
) VALUES (
  'job-uuid',
  'mindfork-roast-coach',
  'v1.0',
  'ft:gpt-4o-mini:mindfork::abc123',
  false,  -- Start in staging
  'staging'
);
```

**Result**: Custom fine-tuned model trained on YOUR users' preferences! üéì

---

## üî• **PRIORITY 2: Episodic Memory (Long-Term User Context)**

### What It Is
Remember key facts about users across sessions (preferences, goals, past conversations)

### Why High ROI
‚úÖ **Personalization**: Coach remembers user's struggles/wins
‚úÖ **Continuity**: "Last time you mentioned..."
‚úÖ **Reduced Repetition**: Don't ask same questions twice

### Table: `ai_episodic_memory`

```sql
-- Already exists! Just needs to be populated
CREATE TABLE IF NOT EXISTS public.ai_episodic_memory (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,

  -- Memory content
  memory_type TEXT NOT NULL, -- 'goal', 'preference', 'achievement', 'struggle', 'milestone'
  memory_text TEXT NOT NULL,
  importance_score NUMERIC DEFAULT 5.0 CHECK (importance_score >= 0 AND importance_score <= 10),

  -- Context
  related_conversation_id UUID,
  extracted_at TIMESTAMPTZ DEFAULT NOW(),
  last_accessed TIMESTAMPTZ,
  access_count INT DEFAULT 0,

  -- Memory lifecycle
  is_active BOOLEAN DEFAULT TRUE,
  expires_at TIMESTAMPTZ,  -- Some memories fade

  created_at TIMESTAMPTZ DEFAULT NOW()
);
```

### Implementation (1 Day)

#### Extract Memories from Conversations

```sql
CREATE OR REPLACE FUNCTION extract_user_memories(
  p_user_id UUID,
  p_conversation_id UUID
)
RETURNS VOID
LANGUAGE plpgsql
AS $$
DECLARE
  v_recent_messages TEXT;
BEGIN
  -- Get recent conversation context
  SELECT STRING_AGG(content, ' ')
  INTO v_recent_messages
  FROM conversations
  WHERE user_id = p_user_id
    AND id = p_conversation_id
    AND message_type = 'user'
  LIMIT 10;

  -- Extract memories using simple keyword matching
  -- (In production, use OpenAI to extract this)

  -- Goal mentions
  IF v_recent_messages ILIKE '%want to%' OR v_recent_messages ILIKE '%goal%' THEN
    INSERT INTO ai_episodic_memory (user_id, memory_type, memory_text, importance_score)
    VALUES (p_user_id, 'goal', v_recent_messages, 8.0);
  END IF;

  -- Preference mentions
  IF v_recent_messages ILIKE '%don''t like%' OR v_recent_messages ILIKE '%prefer%' THEN
    INSERT INTO ai_episodic_memory (user_id, memory_type, memory_text, importance_score)
    VALUES (p_user_id, 'preference', v_recent_messages, 7.0);
  END IF;

  -- Achievement mentions
  IF v_recent_messages ILIKE '%lost%pounds%' OR v_recent_messages ILIKE '%achieved%' THEN
    INSERT INTO ai_episodic_memory (user_id, memory_type, memory_text, importance_score)
    VALUES (p_user_id, 'achievement', v_recent_messages, 9.0);
  END IF;
END;
$$;
```

#### Use Memories in System Prompt

```sql
-- Add to build_coach_system_prompt()
CREATE OR REPLACE FUNCTION get_user_memory_context(p_user_id UUID)
RETURNS TEXT
LANGUAGE plpgsql
AS $$
DECLARE
  v_memories TEXT;
BEGIN
  SELECT STRING_AGG(
    format('- [%s] %s', memory_type, memory_text),
    E'\n'
  )
  INTO v_memories
  FROM (
    SELECT memory_type, memory_text
    FROM ai_episodic_memory
    WHERE user_id = p_user_id
      AND is_active = TRUE
    ORDER BY importance_score DESC, last_accessed DESC NULLS LAST
    LIMIT 5  -- Top 5 most important memories
  ) recent_memories;

  IF v_memories IS NOT NULL THEN
    RETURN format('
KNOWN USER CONTEXT (Remember These):
%s

Reference these memories naturally in conversation when relevant.
', v_memories);
  ELSE
    RETURN '';
  END IF;
END;
$$;
```

**Result**: Coach remembers users across sessions! üß†

---

## üî• **PRIORITY 3: A/B Testing Framework (ai_experiments)**

### What It Is
Test different prompts, severities, coaches ‚Üí See which performs best

### Why Medium ROI
‚úÖ **Data-Driven**: Know what works, not guess
‚úÖ **Continuous Optimization**: Always improving
‚úÖ **User Segmentation**: Different approaches for different users

### Table: `ai_experiments`

```sql
CREATE TABLE IF NOT EXISTS public.ai_experiments (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  experiment_name TEXT NOT NULL,
  description TEXT,

  -- Variants
  control_variant JSONB NOT NULL,  -- Baseline
  test_variants JSONB[] NOT NULL,   -- Alternatives

  -- Metrics to track
  success_metric TEXT NOT NULL,  -- 'thumbs_up_rate', 'avg_rating', 'engagement_time'

  -- Status
  status TEXT DEFAULT 'draft',  -- 'draft', 'running', 'paused', 'completed'
  start_date TIMESTAMPTZ,
  end_date TIMESTAMPTZ,

  -- Results
  winner_variant TEXT,
  confidence_level NUMERIC,

  created_at TIMESTAMPTZ DEFAULT NOW()
);
```

### Example Experiment

```sql
-- Test different severity defaults
INSERT INTO ai_experiments (
  experiment_name,
  description,
  control_variant,
  test_variants,
  success_metric,
  status
) VALUES (
  'Default Severity Optimization',
  'Test whether 2.5 or 3.5 severity leads to better user satisfaction',
  '{"variant_name": "Control (3.0)", "severity": 3.0}',
  ARRAY[
    '{"variant_name": "Gentler (2.5)", "severity": 2.5}',
    '{"variant_name": "More Direct (3.5)", "severity": 3.5}'
  ]::JSONB[],
  'thumbs_up_rate',
  'running'
);
```

**Result**: Scientific optimization of coach behavior! üìä

---

## üìä **ROI Summary Table**

| Feature | Tables Used | Dev Time | Expected Impact | Priority |
|---------|-------------|----------|-----------------|----------|
| **RLHF Training Pipeline** | `ai_training_datasets`, `ai_training_examples`, `ai_finetuning_jobs` | 3 days | üî•üî•üî•üî•üî• | **1 - DO FIRST** |
| **Episodic Memory** | `ai_episodic_memory` | 1 day | üî•üî•üî•üî• | **2 - HIGH VALUE** |
| **Response Feedback** | `coach_response_feedback` | 1 day | üî•üî•üî•üî•üî• | **1 - REQUIRED FOR RLHF** |
| **A/B Testing** | `ai_experiments` | 2 days | üî•üî•üî• | **3 - MEDIUM VALUE** |
| **Context Caching** | `ai_context_cache` | 1 day | üî•üî•üî•üî• | **2 - SPEED BOOST** |
| **Model Versioning** | `ai_model_versions` | 0.5 days | üî•üî• | **4 - NICE TO HAVE** |

---

## üöÄ **7-Day Implementation Roadmap**

### Days 1-3: Response Caching + Severity (Already Planned) ‚úÖ
- Day 1: Response caching
- Days 2-3: Severity system

### Days 4-5: RLHF Foundation üî• **ADD THIS**
- Day 4: Add feedback capture (thumbs up/down)
- Day 5: Auto-generate training datasets from feedback

### Day 6: Episodic Memory üß† **ADD THIS**
- Populate `ai_episodic_memory` from conversations
- Integrate memories into system prompts

### Day 7: Fine-Tuning Prep üéì **ADD THIS**
- Export first training dataset
- Submit fine-tuning job to OpenAI
- Set up weekly automated exports

---

## üí∞ **Expected Value Over 6 Months**

### Without RLHF (Current Plan)
- Cost: $1,000/month in OpenAI API calls
- Quality: Static (doesn't improve)
- Differentiation: Low (anyone can use GPT-4o)

### With RLHF (Proposed)
- **Month 1**: $1,000 (collecting feedback)
- **Month 2**: $1,050 (fine-tuning cost: $50)
- **Month 3-6**: $600/month (fine-tuned model 40% cheaper)
- **Quality**: Improves 10-20% monthly
- **Differentiation**: HIGH (custom model trained on your users)

**Total Savings**: $1,800 over 6 months
**Competitive Moat**: Priceless (your model understands YOUR users better than generic GPT-4o)

---

## ‚úÖ **Recommendation**

**Extend Option A to 10 days**:

- Days 1-3: Cache + Severity + Modes (as planned) ‚úÖ
- **Day 4**: Add thumbs up/down feedback UI üëçüëé
- **Day 5**: Auto-generate training datasets üîÑ
- **Day 6**: Episodic memory integration üß†
- **Day 7**: Fine-tuning pipeline setup üéì
- **Days 8-10**: Buffer for testing + documentation üìù

**Result**: Fully self-improving AI system that gets smarter with every user interaction! üöÄ

**Ready to build the RLHF foundation (Days 4-7)?** üî•
